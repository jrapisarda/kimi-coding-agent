{
  "requirements": {
    "overview": "Build a local-first, multi-agent coding system driven by OpenAI’s Agents SDK and Responses API, optimized for runnability, reproducibility, and current library patterns. The MVP orchestrates four agents (Requirements, Coding, Testing, Documentation) via a CLI, executes code locally with snapshot/rollback safety, and persists rich artifacts to SQLite. Uses GPT-5 via the OpenAI Python SDK (Responses API) and built-in tools as needed (e.g., Code Interpreter; optional Web Search/File Search) with provenance logging. References: OpenAI Agents SDK and Responses API (announced Mar 11, 2025) and AgentKit (Oct 6, 2025). ([openai.com](https://openai.com/index/new-tools-for-building-agents/?utm_source=openai))",
    "goals": [
      "Deliver a runnable CLI that executes an end-to-end multi-agent workflow (Requirements → Coding → Testing → Docs) on a single developer workstation in under 10 minutes for typical reference projects.",
      "Favor up-to-date ecosystem usage by verifying versions/patterns at run time with an optional web_search tool (e.g., Next.js ≥15.x, FastAPI ≥0.118, pytest ≥8.4.x). ([nextjs.org](https://nextjs.org/docs/canary/app/building-your-application/upgrading?utm_source=openai))",
      "Ensure local-first execution with sandboxing, filesystem snapshots, and automatic rollback on unrecoverable errors.",
      "Persist every run’s context, diffs, logs, tests, metrics, and provenance to a local SQLite database for reproducibility and audit.",
      "Integrate OpenAI Python SDK Responses API and Agents SDK for tool-use, structured outputs, and tracing. ([github.com](https://github.com/openai/openai-python?utm_source=openai))"
    ],
    "assumptions": [
      "Target environment: developer laptop/workstation (macOS/Linux/WSL), Python 3.13.x recommended; Python 3.8+ minimum. ([python.org](https://www.python.org/search/?page=21&q=-0++-%2A&utm_source=openai))",
      "Node.js available for FE reference projects; Next.js requires Node.js ≥18.18.0. ([nextjs.org](https://nextjs.org/docs/app/getting-started/installation?utm_source=openai))",
      "OpenAI API key is available via environment variable; business data is not used for training by default per OpenAI policy. ([openai.com](https://openai.com/index/new-tools-for-building-agents/?utm_source=openai))",
      "Internet access may be toggled; when enabled, the agent can call web_search to confirm library docs/patterns.",
      "Licenses for any fetched snippets/templates must be MIT/Apache-2.0/BSD; provenance is recorded.",
      "Primary languages for MVP outputs: Python (backend/ETL/ML) and TypeScript/React for Next.js FE.",
      "SQLite is available locally; no external DB is required for MVP."
    ],
    "scope": {
      "must": [
        "CLI entrypoint: agent run <target_path> [--input_docs <path>] [--prompt \"...\"] with progress, structured errors, and exit codes.",
        "Sequential multi-agent orchestration with context handoffs and error containment (Requirements → Coding → Testing → Documentation).",
        "OpenAI Python SDK integration using Responses API/Agents SDK; enable built-in tools (at minimum Code Interpreter; allow opt-in Web Search/File Search). ([github.com](https://github.com/openai/openai-python?utm_source=openai))",
        "Local sandbox execution with pre-run snapshot and automatic rollback on unrecoverable failure.",
        "SQLite persistence for runs, steps, artifacts, provenance, SBOM, and test results as JSON blobs.",
        "Basic testing via pytest for Python deliverables; auto-generate smoke tests and run them. Use current pytest 8.4.x patterns. ([docs.pytest.org](https://docs.pytest.org/en/stable/changelog.html?utm_source=openai))",
        "Output packaging: create dist/<run_id>.zip including code, tests, README, logs, and metadata.",
        "License guardrails: block non-MIT/Apache/BSD snippets; record URLs/licenses for fetched content."
      ],
      "should": [
        "Determinism aids: capture seeds, environment, versions; write a reproducibility summary.",
        "Fix-forward once on common failures (e.g., dependency re-pin) before rollback.",
        "Progress streaming and concise run summaries.",
        "Minimal Documentation Agent output (README with quickstart, commands, and known limitations).",
        "Security hygiene: redact secrets, validate inputs, and isolate subprocess execution."
      ],
      "wont": [
        "Web UI, multi-tenant features, real-time collaboration, plugin architecture, advanced analytics, or cloud deployment automation in MVP.",
        "Custom fine-tuning; rely on GPT-5 base models initially. ([openai.com](https://openai.com/api/?utm_source=openai))"
      ]
    },
    "user_stories": [
      {
        "role": "Developer",
        "goal": "Generate a Next.js dashboard app scaffold with linting, scripts, and a smoke test from a one-line prompt.",
        "reason": "Quickly bootstrap a FE app with current Next.js 15 patterns and Node ≥18.18.0. ([nextjs.org](https://nextjs.org/docs/app/getting-started/installation?utm_source=openai))"
      },
      {
        "role": "Backend Developer",
        "goal": "Create a FastAPI CRUD service from an entity spec with pytest tests and a Dockerfile.",
        "reason": "Stand up a minimal backend that passes tests and runs locally using FastAPI ≥0.118 and pytest ≥8.4.x. ([pypi.org](https://pypi.org/project/fastapi/?utm_source=openai))"
      },
      {
        "role": "Data Engineer",
        "goal": "Build a local ETL that ingests CSV to SQLite with idempotent loads and a small validation test suite.",
        "reason": "Automate reliable data loading using pandas ≥2.3.x with artifact logs stored in SQLite. ([pandas.pydata.org](https://pandas.pydata.org/docs/index.html?utm_source=openai))"
      },
      {
        "role": "ML Practitioner",
        "goal": "Generate a small scikit-learn training/eval pipeline with saved metrics and a regression test.",
        "reason": "Validate a minimal ML workflow using scikit-learn ≥1.7.x and deterministic seeds. ([scikit-learn.org](https://scikit-learn.org/stable/whats_new/v1.7.html?utm_source=openai))"
      },
      {
        "role": "Operator",
        "goal": "Recover cleanly from a failed run with an automatic rollback and a detailed failure report.",
        "reason": "Avoid corrupted workspaces and speed up debugging."
      }
    ],
    "flows": [
      {
        "name": "End-to-end run (CLI → Orchestration → Packaging)",
        "steps": [
          "User runs: agent run ./target --input_docs ./docs --prompt \"...\"",
          "Requirements Agent parses prompt and docs; emits structured context JSON.",
          "Coding Agent plans and scaffolds code; uses OpenAI Responses API with tool calls (e.g., Code Interpreter). ([openai.com](https://openai.com/index/new-tools-for-building-agents/?utm_source=openai))",
          "Local execution: install deps, run build/test commands, capture outputs.",
          "Testing Agent generates/executes pytest or FE smoke tests; stores reports. ([docs.pytest.org](https://docs.pytest.org/en/stable/changelog.html?utm_source=openai))",
          "Documentation Agent writes README and usage notes.",
          "Persist run graph, artifacts, and provenance to SQLite; emit dist/<run_id>.zip.",
          "Print summary and exit with appropriate code."
        ]
      },
      {
        "name": "Snapshot and rollback",
        "steps": [
          "Before changes, create a filesystem snapshot of target_path.",
          "Apply code changes and run tasks within sandbox.",
          "On unrecoverable failure: stop, restore snapshot, persist diagnostics to SQLite, and exit non-zero."
        ]
      },
      {
        "name": "Version-aware generation (web_search on)",
        "steps": [
          "For each selected stack, query docs for current versions/patterns (e.g., Next.js 15.5, FastAPI 0.118, pytest 8.4.2+). ([nextjs.org](https://nextjs.org/blog?utm_source=openai))",
          "Adjust templates and commands accordingly (e.g., Node ≥18.18, React 19 for Next 15). ([nextjs.org](https://nextjs.org/docs/app/getting-started/installation?utm_source=openai))",
          "Record sources and library versions in provenance manifest."
        ]
      },
      {
        "name": "Reference projects validation loop",
        "steps": [
          "Run each reference project spec through the four-agent pipeline.",
          "Enforce time budget (<10 minutes) and resource caps.",
          "Store metrics (pass/fail, duration, test results) in SQLite and summarize.",
          "Package artifacts and compare against acceptance criteria."
        ]
      }
    ],
    "risks": [
      "OpenAI API rate limits or transient errors can break flows; mitigate with retries, backoff, and queueing.",
      "SDK or API surface changes (Agents/Responses) may require updates; pin SDK versions and add compatibility shims. ([openai.com](https://openai.com/index/new-tools-for-building-agents/?utm_source=openai))",
      "Dependency conflicts across generated projects (e.g., React/Node or Python libs) can cause build failures; use templates aligned to current docs and lockfiles. ([nextjs.org](https://nextjs.org/docs/app/getting-started/installation?utm_source=openai))",
      "Model output variability may yield flaky tests; capture seeds and enable one retry before rollback.",
      "Security exposure if tools fetch untrusted content; enforce license filters, sanitize inputs/outputs, and redact secrets.",
      "Performance regressions on large inputs; enforce per-step timeouts and summarize partial results.",
      "Local environment drift (Python/Node versions) causing non-reproducible runs; record env and warn when below minimum versions. ([python.org](https://www.python.org/search/?page=21&q=-0++-%2A&utm_source=openai))"
    ],
    "open_questions": [
      "Q15: Confirm 3–4 canonical reference projects for MVP validation. Proposed defaults: (1) Next.js 15 dashboard (App Router, TS), (2) FastAPI CRUD service with one entity, (3) CSV↔SQLite ETL (pandas), (4) scikit-learn classification pipeline. Accept? ([nextjs.org](https://nextjs.org/blog/next-15?utm_source=openai))",
      "Confirm which OpenAI built-in tools to enable by default (Code Interpreter only vs. also Web Search and File Search) and any egress/network constraints. ([openai.com](https://openai.com/index/new-tools-for-building-agents/?utm_source=openai))",
      "Target Python version policy: is Python 3.13.x mandatory or is 3.11/3.12 acceptable for broader compatibility? ([python.org](https://www.python.org/search/?page=21&q=-0++-%2A&utm_source=openai))",
      "Node/React policy for FE runs: require Node ≥18.18 and React 19 for Next 15, or allow older React 18 for broader compatibility? ([nextjs.org](https://nextjs.org/docs/app/getting-started/installation?utm_source=openai))",
      "Preferred logging verbosity and artifact retention (SQLite DB growth, zip rotation)—what limits should we set by default?",
      "Do we need Guardrails or enterprise controls (PII masking, jailbreak detection) in MVP or defer to Phase 2? ([openai.com](https://openai.com/index/introducing-agentkit/?utm_source=openai))",
      "Any restrictions on third-party templates or generators beyond MIT/Apache/BSD licensing?"
    ]
  },
  "raw": {
    "prompt": "Build a coding agent using OpenAI SDK",
    "input_docs_text": "# Product Owner Analysis: AI Coding Agent Multi-Agent Orchestration System\n\n**Project:** AI Coding Agent with OpenAI GPT-5 and Multi-Agent Orchestration  \n**Date:** October 7, 2025  \n**Product Owner:** [Jonathan Rapisarda, MBA]  \n**Document Version:** 1.1\n\n## Executive Summary\n\n**APPROVED FOR DEVELOPMENT** with scope modifications and risk mitigation requirements.\n\nThis system represents a sophisticated AI-powered development automation solution using OpenAI's GPT-5 and Agent SDK. The multi-agent orchestration approach aligns with current industry best practices for complex AI workflows, offering significant potential for developer productivity gains.\n\n**Key Decision:** Recommend MVP approach with core functionality first, followed by iterative enhancement based on user feedback and performance metrics.\n\n---\n\n## Stakeholder Story Analysis\n\n### Original Requirements Assessment\n\nThe stakeholder has outlined a **4-agent orchestration system**:\n\n1. **Requirements Analysis Agent** - Document/spec analysis and context preparation\n2. **Coding Agent** - Code execution and testing with code interpreter\n3. **Testing Agent** - PYTEST script generation\n4. **Documentation Agent** - README.md creation and artifact packaging\n\n**Architecture Evaluation:** ✅ **SOUND**  \n- Aligns with proven multi-agent patterns (Sequential Orchestration + Handoff Pattern)\n- Follows software development lifecycle best practices\n- Leverages GPT-5's enhanced coding and tool-calling capabilities\n\n---\n\n## Feature Prioritization Framework\n\n### MUST HAVE (Core MVP - Phase 1)\n\n| Feature | Rationale | Acceptance Criteria |\n|---------|-----------|-------------------|\n| **Requirements Analysis Agent** | Foundation for system operation; critical for context understanding | - Parse common document formats (MD, TXT, JSON)<br>- Extract technical specifications<br>- Generate structured context for coding agent<br>- Handle ambiguous requirements with clarification requests |\n| **Coding Agent with Basic Testing** | Core value proposition; essential for automation | - Generate functional code from requirements<br>- Execute code using code interpreter<br>- Basic error handling and debugging<br>- Support for Python (primary language)<br>- Integration with OpenAI GPT-5 API |\n| **CLI Interface** | Essential for usability and deployment | - Command-line argument parsing<br>- Progress indicators<br>- Error reporting<br>- Basic configuration management |\n| **Agent Orchestration Framework** | System backbone; enables multi-agent coordination | - Sequential agent execution<br>- Context passing between agents<br>- Error propagation and handling<br>- Basic logging and monitoring |\n| **Output Packaging** | Deliverable requirement | - ZIP file generation with all artifacts<br>- Consistent file structure<br>- Basic metadata inclusion |\n| **Database Integration** | Not required for core functionality; future enhancement; use SQLite to store code generated and requirements in JSON format and other fields |\n**Estimated Effort:** 10-12 weeks  \n**Risk Level:** Medium - Well-defined scope with proven technologies\n\n### SHOULD HAVE (Enhancement - Phase 2)\n\n| Feature | Rationale | Dependencies | Timeline |\n|---------|-----------|--------------|----------|\n| **Advanced PYTEST Testing Agent** | Enhances code quality; valuable for professional use | Core MVP completed | +3-4 weeks |\n| **Documentation Agent** | Professional deliverable; improves adoption | Core MVP, Testing Agent | +2-3 weeks |\n| **Multi-Language Support** | Market expansion; competitive advantage | Core MVP stable | +4-6 weeks |\n| **Advanced Error Recovery** | Production readiness; reliability | Core system proven | +2-3 weeks |\n| **Configuration Management** | User experience; customization | CLI interface stable | +1-2 weeks |\n| **Progress Streaming** | User experience; transparency | Basic orchestration working | +2 weeks |\n\n### WON'T HAVE (Out of Scope - Current Release)\n\n| Feature | Rationale for Exclusion |\n|---------|------------------------|\n| **Web UI Interface** | CLI sufficient for MVP; adds complexity without core value |\n| **Custom Model Fine-tuning** | GPT-5 capabilities sufficient; premature optimization |\n| **Multi-tenant Support** | Single-user focus for MVP; enterprise feature |\n| **Real-time Collaboration** | Complex implementation; not core to individual developer workflow |\n| **Plugin Architecture** | Over-engineering for MVP; consider post-validation |\n| **Advanced Analytics/Metrics** | Basic logging sufficient initially; enhancement opportunity |\n| **Cloud Deployment Automation** | Local execution sufficient for MVP |\n\n---\n\n## Technical Architecture Decisions\n\n### Approved Architecture Patterns\n\n1. **Sequential Orchestration with Handoffs** - Agents work in defined order with context passing\n2. **Tool Use Pattern** - Each agent has access to specific tools and APIs\n3. **Reflection Pattern** - Agents can self-audit and improve outputs\n4. **Error Containment** - Failures in one agent don't cascade to others\n\n### Required Technical Stack\n\n- **Primary:** OpenAI GPT-5 + Agent SDK\n- **Language:** Python 3.9+\n- **Testing:** pytest framework integration\n- **Packaging:** Standard Python packaging tools\n- **Documentation:** Markdown generation capabilities\n\n---\n\n## Risk Analysis & Mitigation\n\n### HIGH RISK\n\n| Risk | Impact | Probability | Mitigation Strategy |\n|------|--------|-------------|-------------------|\n| **OpenAI API Rate Limits** | System unusable during high usage | Medium | - Implement rate limiting and queuing<br>- Add retry logic with exponential backoff<br>- Consider API tier upgrade path |\n| **GPT-5 Model Reliability** | Inconsistent output quality | Medium | - Implement output validation<br>- Add fallback strategies<br>- Build comprehensive test suite |\n| **Agent Context Loss** | Poor hand-off between agents | High | - Implement persistent context storage<br>- Add context validation checks<br>- Design recovery mechanisms |\n\n### MEDIUM RISK\n\n| Risk | Impact | Probability | Mitigation Strategy |\n|------|--------|-------------|-------------------|\n| **Complex Requirements Handling** | Poor code generation for ambiguous specs | Medium | - Build requirement validation<br>- Implement clarification request system<br>- Create example-based learning |\n| **Testing Agent Accuracy** | Poor test coverage or quality | Medium | - Validate against known test patterns<br>- Implement test quality metrics<br>- Add human review checkpoints |\n\n---\n\n## Acceptance Criteria & Definition of Done\n\n### System-Level Acceptance Criteria\n\n1. **End-to-End Functionality**\n   - [ ] System processes requirements document (MD/TXT) successfully\n   - [ ] Generates functional code that executes without critical errors\n   - [ ] Produces ZIP package with all artifacts\n   - [ ] Completes full workflow in under 10 minutes for typical projects\n\n2. **Quality Standards**\n   - [ ] Generated code passes basic syntax validation\n   - [ ] System handles common error scenarios gracefully\n   - [ ] Output includes appropriate logging and error messages\n   - [ ] Performance meets baseline requirements (documented)\n\n3. **Usability Requirements**\n   - [ ] CLI interface intuitive for developers\n   - [ ] Documentation sufficient for self-service adoption\n   - [ ] Error messages actionable and clear\n\n### Definition of Done (DoD)\n\n- [ ] All MUST HAVE features implemented and tested\n- [ ] Unit tests achieve 80%+ code coverage\n- [ ] Integration tests validate end-to-end workflows\n- [ ] Performance benchmarking completed\n- [ ] Security review completed (API key handling, input validation)\n- [ ] Documentation complete (README, API docs, examples)\n- [ ] User acceptance testing completed with 3+ test scenarios\n\n---\n\n## Success Metrics & KPIs\n\n### Primary Success Metrics\n\n| Metric | Target | Measurement Method |\n|--------|--------|--------------------|\n| **System Reliability** | 95% successful completions | Automated logging/monitoring |\n| **Code Quality** | 80% generated code executes without modification | Static analysis + execution testing |\n| **User Satisfaction** | 4.0+ rating (5-point scale) | User surveys post-trial |\n| **Performance** | <10 minutes average end-to-end execution | Automated timing metrics |\n\n### Secondary Metrics\n\n- Time-to-value for new users (<30 minutes to first success)\n- Error recovery rate (90%+ of failures recovered automatically)\n- Documentation accuracy (measured through user feedback)\n\n---\n\n## Dependencies & Prerequisites\n\n### External Dependencies\n\n1. **OpenAI GPT-5 API Access** - CRITICAL PATH\n   - Requires API tier sufficient for expected usage\n   - Rate limits and billing considerations\n   - Model availability and performance SLA\n\n2. **OpenAI Agent SDK Stability** - HIGH RISK\n   - SDK is relatively new (2025 release)\n   - Monitor for breaking changes\n   - Establish version pinning strategy\n\n### Internal Dependencies\n\n1. **Development Team Expertise** - Requirement for multi-agent system experience\n2. **Testing Infrastructure** - CI/CD pipeline for complex AI system testing\n3. **User Research** - Target user interviews for requirements validation\n\n---\n\n## Implementation Roadmap\n\n### Phase 1: Core MVP (Weeks 1-10)\n- Requirements Analysis Agent\n- Basic Coding Agent\n- CLI Interface\n- Simple Orchestration\n- Output Packaging\n\n### Phase 2: Enhancement (Weeks 11-16)\n- PYTEST Testing Agent\n- Documentation Agent\n- Advanced Error Handling\n- Performance Optimization\n\n### Phase 3: Polish & Scale (Weeks 17-20)\n- User Experience Improvements\n- Performance Tuning\n- Documentation Enhancement\n- Launch Preparation\n\n---\n\n## Stakeholder Communication Plan\n\n### Regular Checkpoints\n\n- **Weekly:** Technical progress review\n- **Bi-weekly:** Stakeholder demo with working system\n- **Monthly:** Metrics review and roadmap adjustment\n\n### Decision Points\n\n1. **Week 4:** Architecture validation checkpoint\n2. **Week 8:** MVP functionality review\n3. **Week 12:** Phase 2 go/no-go decision\n4. **Week 16:** Launch readiness assessment\n\n---\n\n## Recommendations\n\n### Immediate Actions Required\n\n1. **Secure OpenAI GPT-5 API Access** - Establish account, rate limits, and billing\n2. **Prototype Core Agent** - Build simple proof-of-concept within 2 weeks\n3. **Define Success Metrics** - Establish baseline measurements\n4. **User Research** - Interview 5+ target users for requirement validation\n\n### Strategic Considerations\n\n1. **Market Positioning** - This system targets experienced developers; ensure complexity is justified by value\n2. **Competitive Landscape** - Monitor similar solutions (Cursor, GitHub Copilot, Windsurf) for feature parity\n3. **Scalability Planning** - Design for future multi-user and cloud deployment scenarios\n\n---\n\n## Conclusion\n\n**APPROVED FOR DEVELOPMENT** with the recommended phased approach. The technical foundation is sound, market opportunity exists, and the risk profile is manageable with proper mitigation strategies.\n\n**Key Success Factors:**\n1. Focus on reliability over feature richness in MVP\n2. Invest heavily in error handling and user experience\n3. Maintain flexibility for rapid iteration based on user feedback\n\n**Next Steps:**\n1. Finalize technical specifications with engineering team\n2. Establish success metrics and measurement infrastructure\n3. Begin Phase 1 development with 2-week proof-of-concept milestone\n\n---\n\n*Document prepared by Product Owner*  \n*For questions or clarification, contact: [contact information]*\n\n**Q15. Canonical acceptance projects:** Which 3–4 “reference projects” should the MVP use for end-to-end validation (e.g., a Next.js FE app, a FastAPI BE service with one CRUD entity, a local ETL job that reads/writes CSV↔SQLite, and a small ML workflow that trains/evaluates a model)?\nIf you don’t have preferences, I’ll pick and document defaults.\n\n---\n\n# RFC.md — AI Coding Agent (Requirements First Pass)\n\n## Overview\n\nAn autonomous, enterprise-capable local development agent that can plan, generate, run, and validate software across FE/BE/ETL and AI/ML project types. It is optimized for “runnability,” leverages current library documentation via a `web_search` tool, and stores rich run artifacts locally. Multi-agent orchestration (Requirements → Coding → Testing → Docs) and a CLI entrypoint drive the MVP, aligning with prior stakeholder notes.  \n\n## Goals\n\n* Deliver a **fully local**, developer-first agent that can autonomously create runnable projects and artifacts with minimal ceremony.\n* Prefer **up-to-date library usage** to avoid deprecated or cyclic dependencies, using `web_search` when enabled.\n* Provide **transparent run history** and reproducibility aids (snapshots, logs, JSON contexts) without requiring cloud services.\n* Implement the **four-agent orchestration** and simple CLI MVP outlined in product owner notes.  \n\n## Assumptions\n\n* Runs on a single developer workstation (10-core CPU @ ~4.4GHz, 32GB RAM, SSD).\n* Everything is local by default; optional **`web_search`** may access the public internet to fetch docs/snippets.\n* The **Architect Agent** performs security review; MVP focuses on runnability and current-docs correctness.\n* The agent may send local source/code snippets to GPT-5 for analysis/generation.\n* Licensing allowed for fetched code/templates: **MIT/Apache-2.0/BSD only**.\n* CLI style: `agent run <path> --input_docs <dir_or_file> --prompt \"<text>\"`. No config file; defaults live in the main file.\n* Stochastic ML behavior is acceptable; summarize outcomes and seeds where used.\n* Local artifact persistence: **SQLite** database with JSON blobs of requirements context and generated code.\n  (Notes confirm single-user, CLI MVP emphasis and SQLite mention.)  \n\n## Requirements\n\n### Must\n\n1. **CLI MVP**: Provide `agent run <path> --input_docs --prompt` with helpful errors and progress output. \n2. **Multi-agent Orchestration**: Sequential handoffs (Requirements → Coding → Testing → Docs), context passing, and failure isolation. \n3. **Autonomous Execution**: Full autonomy within a sandbox: create/modify files, install deps, run builds/tests.\n4. **Runnability First**: Prefer most current library patterns (validated via `web_search`) to avoid deprecated APIs.\n5. **Local-only by Default**: No cloud dependencies; optional `web_search`/OpenAI calls permitted.\n6. **Snapshot & Rollback**: Create pre-run workspace snapshot; auto-rollback on unrecoverable failure.\n7. **Run Artifacts**: Persist structured action log, plan, execution timeline, diffs, test reports, SBOM/provenance, and metadata into **SQLite** (tables for runs, steps, artifacts; JSON fields for requirements & code).\n8. **Testing**: Generate and run basic tests/smoke checks (unit/integration appropriate to project type). Notes call for pytest in the stack. \n9. **Packaging**: Produce a deliverable (e.g., `dist/<run_id>.zip`) with code, tests, docs, and metadata. \n10. **Licensing Guard**: Fetch/emit only MIT/Apache-2.0/BSD-licensed snippets; record provenance in artifacts.\n\n### Should\n\n1. **Resource Governance** (defaults for the stated machine): max 8 cores, ~24GB RAM ceiling, disk writes <10GB/run, soft 15-minute task budget with per-step timeouts.\n2. **Determinism Aids**: Where feasible, capture seeds and environment to ease repro; summarize stochastic variance.\n3. **Doc Generation**: Minimal README and usage notes per run (full documentation agent can expand later). \n4. **Performance Targets**: Aim for <10 minutes E2E on “typical” sample projects; <30 minutes to first success for new users.  \n5. **Error Recovery**: Attempt fix-forward once before rollback when safe (e.g., dependency re-pin, re-gen a file).\n6. **Provenance Manifest**: Include SBOM, dependency versions, and any fetched URL/license markers.\n\n### Won’t (MVP)\n\n* Web UI, multi-tenant features, real-time collaboration, plugin architecture, advanced analytics, cloud deploy automation, custom fine-tuning. (Explicitly deferred in notes.) \n\n## User Stories\n\n1. **Solo Dev—Greenfield FE**\n   As a solo dev, I run `agent run ./apps/fe --prompt \"Create a Next.js app with a dashboard\"` and get a runnable FE with install/build/test scripts, plus a README and a zip deliverable.\n2. **Backend Service—CRUD**\n   As a backend dev, I point `--input_docs` at an entity spec and receive a FastAPI/Express service with one CRUD resource, tests, and a packaged artifact.\n3. **ETL—Local Files ↔ SQLite**\n   As a data engineer, I provide a CSV schema and prompt; the agent generates a local ETL script (e.g., Python + pandas) with idempotent loads into SQLite, with tests and logs stored in the run DB.\n4. **ML Experiment—Local Training**\n   As an ML practitioner, I request a small classifier/regressor pipeline (scikit-learn). The agent trains/evaluates locally, logs metrics/artifacts, and summarizes stochastic outcomes.\n5. **Failure & Rollback**\n   As a developer, if a run fails after partial writes, the agent auto-rolls back to the pre-run snapshot and stores diagnostics in SQLite for inspection.\n\n## Example Flows\n\n### Flow A — FE Project from Prompt\n\n1. User: `agent run ./apps/fe --prompt \"Next.js dashboard with auth stub\"`\n2. Requirements Agent parses prompt; Coding Agent scaffolds FE using current docs via `web_search`.\n3. Testing Agent runs smoke tests; Docs Agent emits README.\n4. Artifacts stored in SQLite; deliverable zip created; summary printed.\n\n### Flow B — BE CRUD from Input Docs\n\n1. User: `agent run ./apps/api --input_docs ./specs/todos.md --prompt \"One CRUD resource\"`\n2. Agents generate FastAPI/Express service with routes, models, tests.\n3. On dependency conflict, agent re-pins based on latest docs; if unresolved, fix-forward → rollback.\n4. Results zipped; SBOM and provenance logged.\n\n### Flow C — ETL to SQLite\n\n1. User: `agent run ./jobs/etl --input_docs ./specs/etl.md --prompt \"CSV→SQLite nightly import\"`\n2. Coding Agent produces idempotent ETL; Testing Agent runs sample data.\n3. Execution timeline, logs, and code JSON stored in the SQLite run DB.\n\n## Edge Cases & Constraints\n\n* **No network**: If `web_search` disabled/unavailable, fall back to local heuristics and pinned templates.\n* **Large repos**: Respect resource caps; skip/vendor huge directories (`node_modules/`, `.venv/`, build artifacts).\n* **Sensitive files**: Never emit contents of `.env`, `secrets/` in artifacts; redact if referenced.\n* **License mismatch**: Block non-MIT/Apache/BSD content; suggest alternatives.\n* **Long-running steps**: Enforce per-step timeouts; summarize partial work if aborted.\n* **Artifact growth**: Rotate/compact SQLite and zip artifacts; keep last N runs (configurable later).\n\n## Metrics & Acceptance (MVP)\n\n* **Reliability**: ≥95% successful completions on reference projects; **Performance**: <10 minutes per “typical” run. \n* **First Success**: New user reaches a working artifact in <30 minutes. \n* **Quality**: Generated code executes without critical errors; tests pass; readable README produced. \n\n---\n\nIf you answer **Q15** (which reference projects you want), I’ll plug them into the “Example Flows,” finalize acceptance criteria per project, and tighten the Must/Should details accordingly.\n"
  }
}